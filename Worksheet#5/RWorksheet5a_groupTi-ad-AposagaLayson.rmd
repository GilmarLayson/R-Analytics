---
title: "RWorksheet#5a"
author: "Ti-ad, Aposaga, Layson"
date: "2024-12-02"
output: pdf_document
---
#IMDB
#1
```{r}
library(rvest)
library(httr)
library(parallelly)
library(dplyr)
library(polite)
```

```{r}
# Specifying the url for desired website to be scraped
url <- 'https://www.imdb.com/chart/toptv/?ref_=nv_tvv_250'
# asking permission to scrape
session <- bow(url, user_agent = "Educational")
session

```

```{r}
# creating objects for the dataset
rank_title <- character(0)
links <- character(0)
```

```{r}
# title list
title_list <- scrape(session) %>%
  html_nodes("h3.ipc-title__text") %>% 
  html_text

title_list

class(title_list)

title_list_sub <- as.data.frame(title_list[2:26])
head(title_list_sub)
tail(title_list_sub)
title_list_sub
```

```{r}
# Extract Show Titles
show_titles <- webpage %>%
  html_nodes('h3.ipc-title__text') %>%
  html_text()

show_titles <- show_titles[show_titles != "IMDb Charts"]
```

```{r}
# Extract Ratings
show_ratings <- webpage %>% 
  html_nodes("span.ipc-rating-star--rating") %>%
  html_text()
```

```{r}
# Extract the number of people who voted
num_votes <- webpage %>%
  html_nodes("span.ipc-rating-star--voteCount") %>%
  html_text()
```

```{r}
# Extract Episode Numbers
episode_info <- webpage %>%
  html_nodes('span.sc-300a8231-7.eaXxft.cli-title-metadata-item:nth-of-type(2)') %>%
  html_text()
```

```{r}
# Clean the episode data (extract only the number of episodes)
episode_counts <- str_extract(episode_info, "\\d+ eps")
episode_counts <- str_remove(episode_counts, " eps")
```

```{r}
# Extract Year of release
year_info <- webpage %>%
  html_nodes('span.sc-300a8231-7.eaXxft.cli-title-metadata-item') %>%
  html_text()
```

```{r}
# Extract the release year using a regex
release_years <- str_extract(year_info, "\\d{4}")
release_years <- release_years[!is.na(release_years)]  # Remove NA values
release_years <- as.numeric(release_years)
```

```{r}
# Function to scrape critic reviews for each show
get_critic_reviews <- function(link) {
  complete_link <- paste0("https://imdb.com", link)
  show_page <- read_html(complete_link)
  
  # Extract critic reviews
  critic <- show_page %>%
    html_nodes("span.score") %>%  # Adjust this if necessary based on page structure
    html_text()
  
  # Return the second critic review (if available)
  if (length(critic) > 1) {
    return(critic[2])  # Take the second item for the critic score
  } else {
    return(NA)  # If no critic review is found
  }
}
```

```{r}
# Function to scrape popularity rating for each show
get_popularity_rating <- function(link) {
  complete_link <- paste0("https://imdb.com", link)
  show_page <- read_html(complete_link)
  
  # Extract popularity rating
  pop_rating <- show_page %>%
    html_nodes('[data-testid="hero-rating-bar__popularity__score"]') %>%
    html_text()
  
  # Return the popularity rating (if available)
  if (length(pop_rating) > 1) {
    return(pop_rating[2])  # The second item should be the popularity rating
  } else {
    return(NA)  # If no popularity rating is found
  }
}

```

```{r}
# Extract links to each show's page
links <- webpage %>%
  html_nodes("a.ipc-title-link-wrapper") %>%
  html_attr("href")
```

```{r}
# Loop through each show's link and get the critic reviews
critic_reviews <- sapply(links, get_critic_reviews)

```

```{r}
# Loop through each show's link and get the popularity rating
popularity_ratings <- sapply(links, get_popularity_rating)
```

```{r}
# Ensure all data has the same length before combining
max_length <- max(length(show_titles), length(show_ratings), length(num_votes), length(episode_counts), length(release_years), length(critic_reviews), length(popularity_ratings))
show_titles <- rep(show_titles, length.out = max_length)
show_ratings <- rep(show_ratings, length.out = max_length)
num_votes <- rep(num_votes, length.out = max_length)
episode_counts <- rep(episode_counts, length.out = max_length)
release_years <- rep(release_years, length.out = max_length)
critic_reviews <- rep(critic_reviews, length.out = max_length)
popularity_ratings <- rep(popularity_ratings, length.out = max_length)
```

```{r}
# Combine into a data frame
imdb_top_tv_shows <- data.frame(
  Show_Title = show_titles,
  Rating = show_ratings,
  Votes = num_votes,
  Episode_Count = episode_counts,
  Release_Year = release_years,
  Critic_Reviews = critic_reviews,
  Popularity_Rating = popularity_ratings,
  stringsAsFactors = FALSE
)

top_50_shows <- imdb_top_tv_shows %>%
  slice(1:50)  # Get the top 50 shows

```

```{r}
# Print the top 50 shows
print(top_50_shows)

write.csv(top_50_shows, "Top_50_shows_num1.csv")

```

```{r}
# ranks
colnames(title_list_sub) <- "ranks"

```

```{r}
# split the string(rank and title)
split_df <- strsplit(as.character(title_list_sub$ranks),".",fixed = TRUE)
split_df <- data.frame(do.call(rbind,split_df))

```

```{r}
# rename and delete columns
# deleting columns 3 and 4 since it duplicated the columns
split_df <- split_df[-c(3:4)] 

```

```{r}
# renaming column 1 and 2
colnames(split_df) <- c("Ranks","Title") 

```

```{r}
# structure of splif_df
str(split_df) 
head(split_df)
split_df

rank_title <- data.frame(
  rank_title = split_df)

write.csv(rank_title,file = "title.csv")

```

```{r}
# Extracting link
# extracting for link of every movie
link_list <- scrape(session) %>%
  html_nodes('a.ipc-title-link-wrapper') %>% 
  html_attr('href') 

```

```{r}
# Ensure no NA values
link_list <- link_list[!is.na(link_list) & link_list != ""]

```

```{r}
# Construct full URLs
for (i in 1:length(link_list)) {
  link_list[i] <- paste0("https://imdb.com", link_list[i])
}
```

```{r}
# Convert to a dataframe
links <- as.data.frame(link_list)
names(links) <- "link"

rank_title <- data.frame(
  rank_title = split_df)

```

```{r}
# combining the dataframe
scrape_df <- data.frame(rank_title)
names(scrape_df) <- c("Rank", "Title")

head(scrape_df)
scrape_df
write.csv(scrape_df,file = "top250.csv")

```

```{r}
# Additional Extraction
current_row <- 1
imdb_top_25 <- data.frame()
```

```{r}
# for this example, we will get only the content for the 1st twenty-five rows
for (row in 1:25) {
  # Get the URL from the "href" column
  url <- links$link[current_row]
  
  # Skip if the URL is empty or improperly constructed
  if (url == "" || is.na(url)) {
    next
  }
  
  # Read the HTML content of the webpage 
  session2 <- bow(url, user_agent = "Educational")
  webpage <- scrape(session2)
  
  # Extract the rating using the appropriate CSS selector
  rating <- html_text(html_nodes(webpage, ".sc-d541859f-1.imUuxf"))
  if (length(rating) == 0) {
    next
  } else {
    rating <- rating[1]
  }
  
  # Extracting votecount
  votecount <- html_text(html_nodes(webpage, 'div.sc-d541859f-3.dwhNqC'))
  if (length(votecount) == 0) {
    next
  } else {
    votecount <- votecount[1]
  }
  
  # Extracting number of episodes and year released
  numofEps_year <- html_text(html_nodes(webpage, xpath = "//span[contains(text(), 'episodes')]"))
  if (length(numofEps_year) == 0) {
    next
  } else {
    numofEps_year <- numofEps_year[1]
    numofEps <- sub("episodes.*", "episodes", numofEps_year)
    year_released <- sub(".*episodes â€¢ ", "", numofEps_year)
  }
  
  # Print or save the extracted data
  cat("Rating for", url, "is:", rating, "vote count is", votecount, "number of episodes is", numofEps, "year released is", year_released, "\n")
  
  # Store the results
  imdb_top_25[current_row, 1] <- rating
  imdb_top_25[current_row, 2] <- votecount
  imdb_top_25[current_row, 3] <- numofEps
  imdb_top_25[current_row, 4] <- year_released
  
  # Move to the next row
  current_row <- current_row + 1
  
  # Add some delay to avoid overloading the server (optional)
  Sys.sleep(3)
}

names(imdb_top_25) <- c("Rating","VoteCount","Number of Episodes", "Year Released")

write.csv(imdb_top_25,file = "imdb_top_25.csv")
```

```{r}
# Combine with the previous dataframe
imdb_top_25 <- data.frame(
  scrape_df, imdb_top_25)

write.csv(imdb_top_25,file = "imdb_top_250.csv")
```

```{r}
# Displaying with kableExtra
library(kableExtra)

knitr::kable(imdb_top_25,caption = "Extracting Rating, VoteCount, Number of Episodes, Year Released") %>%
  kable_classic(full_width = T, html_font = "Cambria") %>%
  kable_styling(font_size = 8)

```

```{r}
# Display top 25 shows only
library(kableExtra)

df_d <- imdb_top_25[c(1:25),]

df_d <- df_d %>%
  select_if(~ !all(is.na(.)))

knitr::kable(df_d, caption = "IMDB Top 25 Shows") %>%
  kable_classic(full_width = T, html_font = "Arial Narrow") %>%
  kable_styling(font_size = 8)

```

#2
```{r}

url2 <- "https://www.imdb.com/title/tt0903747/reviews/?ref_=tt_urv_sm"
url_session <- bow(url2, user_agent = "Educational")
movie_link1 <- "https://www.imdb.com/title/tt0903747/reviews/?ref_=tt_urv_sm"

get_reviewer_name <- function(movie_link) {
  movie_page <- read_html(movie_link)
  tv_reviewer_name <- movie_page %>%
    html_nodes("a.ipc-link.ipc-link--base") %>%
    html_text()
}

reviewer_name <- sapply(movie_link1, FUN = get_reviewer_name)

reviewNAME <- as.data.frame(reviewer_name[2:6])
reviewNAME

```

```{r}
library(dplyr)
library(rvest)
library(stringr)

url <- "https://www.imdb.com/chart/toptv/?sort=rank"
page <- read_html(url)


ranked_titles <- page %>%
  html_nodes(".titleColumn a") %>%
  html_text()
```

```{r}
title_data <- as.data.frame(ranked_titles[3:27], stringsAsFactors = FALSE)
colnames(title_data) <- "ranked_titles"
```

```{r}

split_titles <- strsplit(as.character(title_data$ranked_titles), "\\.", fixed = FALSE)
```


```{r}
title_df <- do.call(rbind, split_titles)
```

```{r}
if (ncol(title_df) == 2) {
  colnames(title_df) <- c("rank", "title")
} else {
  title_df <- data.frame(rank = rep(NA, length(split_titles)), 
                         title = as.character(split_titles))
}
```

```{r}
title_df$title <- trimws(title_df$title)
```

```{r}

tv_show_links <- paste0("https://www.imdb.com/title/", 
                        c("tt0903747", "tt5491994", "tt0795176", "tt0185906", "tt7366338"), 
                        "/reviews/?ref_=tt_ov_urv")
```

```{r}

scrape_reviews <- function(link, desired_rows = 20) {
  page <- read_html(link)
  # Extract review details
  name <- page %>% html_nodes(".ipc-link.ipc-link--base") %>% html_text()
  year <- page %>% html_nodes(".ipc-inline-list__item.review-date") %>% html_text()
  rating <- page %>% html_nodes(".ipc-rating-star--rating") %>% html_text()
  title <- page %>% html_nodes(".ipc-title__text") %>% html_text()
  helpful <- page %>% html_nodes(".ipc-voting__label__count.ipc-voting__label__count--up") %>% html_text()
  unhelpful <- page %>% html_nodes(".ipc-voting__label__count.ipc-voting__label__count--down") %>% html_text()
  text <- page %>% html_nodes(".ipc-html-content-inner-div") %>% html_text()

  name <- c(name, rep(NA, max(0, desired_rows - length(name))))
  year <- c(year, rep(NA, max(0, desired_rows - length(year))))
  rating <- c(rating, rep(NA, max(0, desired_rows - length(rating))))
  title <- c(title, rep(NA, max(0, desired_rows - length(title))))
  helpful <- c(helpful, rep(NA, max(0, desired_rows - length(helpful))))
  unhelpful <- c(unhelpful, rep(NA, max(0, desired_rows - length(unhelpful))))
  text <- c(text, rep(NA, max(0, desired_rows - length(text))))
  
  name <- gsub("Permalink", "ANONYMOUS", name)
  name <- str_trim(name) 
  
  name <- c(name, rep(NA, max(0, desired_rows - length(name))))
  year <- c(year, rep(NA, max(0, desired_rows - length(year))))
  rating <- c(rating, rep(NA, max(0, desired_rows - length(rating))))
  title <- c(title, rep(NA, max(0, desired_rows - length(title))))
  helpful <- c(helpful, rep(NA, max(0, desired_rows - length(helpful))))
  unhelpful <- c(unhelpful, rep(NA, max(0, desired_rows - length(unhelpful))))
  text <- c(text, rep(NA, max(0, desired_rows - length(text))))
  
  name <- name[1:desired_rows]
  year <- year[1:desired_rows]
  rating <- rating[1:desired_rows]
  title <- title[1:desired_rows]
  helpful <- helpful[1:desired_rows]
  unhelpful <- unhelpful[1:desired_rows]
  text <- text[1:desired_rows]
  
  reviews <- data.frame(
    name = name,
    year = year,
    rating = rating,
    title = title,
    helpful = helpful,
    unhelpful = unhelpful,
    text = text,
    stringsAsFactors = FALSE
  )
  
  return(reviews)
}




```

```{r}
tv_show_links <- paste0("https://www.imdb.com/title/", 
                        c("tt0903747", "tt5491994", "tt0795176", "tt0185906", "tt7366338"), 
                        "/reviews/?ref_=tt_ov_urv")

all_reviews <- lapply(tv_show_links, scrape_reviews, desired_rows = 20)


combined_reviews <- do.call(rbind, all_reviews)
print(combined_reviews)
```

```{r}
write.csv(combined_reviews, file = "movie_reviews.csv")
```

```{r}
all_reviews <- lapply(tv_show_links, function(link) scrape_reviews(link, 20))
final_reviews <- bind_rows(all_reviews, .id = "tv_show_id")
final_reviews <- final_reviews %>%
  mutate(tv_show_title = title_df$title[as.integer(tv_show_id)], 
         rank = title_df$rank[as.integer(tv_show_id)])
final_reviews <- final_reviews %>%
  mutate(tv_show_title = title_df$title[as.integer(tv_show_id)], 
         rank = title_df$rank[as.integer(tv_show_id)])
final_reviews <- final_reviews %>%
  select(-helpful, -unhelpful, -tv_show_title, -rank)

print(final_reviews)
```

```{r}
write.csv(final_reviews, file = "movie_reviews_final.csv")
```

```{r}
write.csv(combined_reviews, file = "movie_reviews.csv")
```

```{r}
all_reviews <- lapply(tv_show_links, function(link) scrape_reviews(link, 20))
final_reviews <- bind_rows(all_reviews, .id = "tv_show_id")
final_reviews <- final_reviews %>%
  mutate(tv_show_title = title_df$title[as.integer(tv_show_id)], 
         rank = title_df$rank[as.integer(tv_show_id)])
final_reviews <- final_reviews %>%
  mutate(tv_show_title = title_df$title[as.integer(tv_show_id)], 
         rank = title_df$rank[as.integer(tv_show_id)])
final_reviews <- final_reviews %>%
  select(-helpful, -unhelpful, -tv_show_title, -rank)

print(final_reviews)
```

```{r}
write.csv(final_reviews, file = "movie_reviews_final.csv")
```

#3
```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Count the number of TV shows released per year
haha <- imdb_top_tv_shows %>%
  group_by(Release_Year) %>%
  summarise(Number_of_Shows = n())

# Plot the time series graph
ggplot(haha, aes(x = Release_Year, y = Number_of_Shows)) +
  geom_line() +  # Add a line plot
  geom_point() +  # Add points at each data point
  labs(title = "Number of TV Shows Released by Year",
       x = "Year",
       y = "Number of TV Shows Released") +
  theme_minimal()

```

#Amazon
#number 4. and 5.
```{r}
library(polite)
polite::use_manners(save_as = 'polite_scrape.R')

url <- 'https://www.amazon.com'


session <- bow(url,
               user_agent = "Educational")
session
```

```{r}
library(rvest)
library(dplyr)
library(stringr)


scrape_amazon_products <- function(base_url, category, num_products = 30) {
  all_data <- data.frame()
  page_number <- 1
  
  while (nrow(all_data) < num_products) {
    # Construct the URL for the current page
    url <- paste0(base_url, "&page=", page_number)
    message("Scraping: ", url)
    
    page <- read_html(url)
    
    product_titles <- page %>%
      html_nodes("span.a-text-normal") %>% 
      html_text(trim = TRUE)
    
    product_titles <- product_titles[product_titles != "Check each product page for other buying options."]

    price <- page %>% 
      html_nodes('.a-price .a-offscreen') %>% 
      html_text(trim = TRUE)
    
    ratings <- page %>% 
      html_nodes('span.a-icon-alt') %>% 
      html_text(trim = TRUE) %>%
      str_extract("\\d\\.\\d") %>%  
      as.numeric()
    


    reviews <- page %>%
      html_nodes('.s-link-style .s-underline-text') %>% 
      html_text(trim = TRUE)
    
    descriptions <- page %>%
      html_nodes("span.a-text-normal") %>% 
      html_text(trim = TRUE)
    
    descriptions <- descriptions[descriptions != "Check each product page for other buying options."]
    
    min_length <- min(length(product_titles), length(price), length(ratings), length(descriptions), length(reviews))
    if (min_length == 0) break  # Exit if no products are found on the page
    
    data <- data.frame(
      ProductTitle = head(product_titles, min_length),
      Price = head(price, min_length),
      Category = rep(category, min_length),
      Ratings = head(ratings, min_length),
      ReviewCount = head(reviews, min_length),
      Description = head(descriptions, min_length)
    )
    
    
    all_data <- bind_rows(all_data, data)
    
    page_number <- page_number + 1
  }
  
  all_data <- head(all_data, num_products)
  

  all_data$ProductTitle <- paste0(seq_len(nrow(all_data)), ". ", all_data$ProductTitle)
  
  return(all_data)
}

artscrafts_url <- "https://www.amazon.com/s?i=specialty-aps&bbn=4954955011&rh=n%3A4954955011%2Cn%3A%212617942011%2Cn%3A2747968011&ref=nav_em__nav_desktop_sa_intl_painting_drawing_supplies_0_2_8_2"
boyclothing_url <- "https://www.amazon.com/s?i=specialty-aps&bbn=16225021011&rh=n%3A7141123011%2Cn%3A16225021011%2Cn%3A1040666&ref=nav_em__nav_desktop_sa_intl_clothing_0_2_15_2"
comptabs_url <- "https://www.amazon.com/s?i=specialty-aps&bbn=16225007011&rh=n%3A16225007011%2Cn%3A13896617011&ref=nav_em__nav_desktop_sa_intl_computers_tablets_0_2_6_4"
dogsupplies_url <- "https://www.amazon.com/s?i=specialty-aps&bbn=16225013011&rh=n%3A%2116225013011%2Cn%3A2975312011&ref=nav_em__nav_desktop_sa_intl_dogs_0_2_21_2"
girlclothing_url <- "https://www.amazon.com/s?i=specialty-aps&bbn=16225020011&rh=n%3A7141123011%2Cn%3A16225020011%2Cn%3A1040664&ref=nav_em__nav_desktop_sa_intl_clothing_0_2_14_2"

artscraftsproducts <- scrape_amazon_products(artscrafts_url, "Arts and Crafts", 30)
boyclothingproducts <- scrape_amazon_products(boyclothing_url, "Boys' Clothing", 30)
comptabsproducts <- scrape_amazon_products(comptabs_url, "Computers and Tablets", 30)
dogsuppliesproducts <- scrape_amazon_products(dogsupplies_url, "Dog Supplies", 30)
girlclothingproducts <- scrape_amazon_products(girlclothing_url, "Girls' Clothing", 30)

all_products <- bind_rows(artscraftsproducts, boyclothingproducts, comptabsproducts, dogsuppliesproducts, girlclothingproducts)

all_products

write.csv(all_products, "Amazon_products.csv")
```

#6. We scraped 30 amazon products from 5 different categories. We then made a csv file out of that data, that displays their Product Titles, Price, Description, Ratings and Reviews.

#7. The use case for this data would be market research.

#8. The graphs show pricing trends, customer satisfaction, and product popularity, helping identify competitive prices and best-selling products. They also highlight links between ratings and reviews, showing which products are highly rated and frequently bought.

```{r}
library(ggplot2)
library(dplyr)

# Load data
all_products <- read.csv("Amazon_products.csv")

# Clean up price data (remove $ and convert to numeric)
all_products$Price <- as.numeric(gsub("[$,]", "", all_products$Price))

# Filter out NA or missing data
all_products <- all_products %>%
  filter(!is.na(Price), !is.na(Ratings), !is.na(ReviewCount))

# Graph 1: Price Distribution
ggplot(all_products, aes(x = Price)) +
  geom_histogram(binwidth = 50, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Product Prices", x = "Price ($)", y = "Count") +
  theme_minimal()

# Graph 2: Average Ratings by Category
avg_ratings <- all_products %>%
  group_by(Category) %>%
  summarize(AverageRating = mean(Ratings, na.rm = TRUE))

ggplot(avg_ratings, aes(x = Category, y = AverageRating, fill = Category)) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Average Ratings by Category", x = "Category", y = "Average Rating") +
  theme_minimal()

# Graph 3: Number of Reviews by Category
total_reviews <- all_products %>%
  group_by(Category) %>%
  summarize(TotalReviews = sum(as.numeric(gsub("[^0-9]", "", ReviewCount)), na.rm = TRUE))

ggplot(total_reviews, aes(x = Category, y = TotalReviews, fill = Category)) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Total Number of Reviews by Category", x = "Category", y = "Total Reviews") +
  theme_minimal()

# Graph 4: Correlation Between Ratings and Reviews
ggplot(all_products, aes(x = Ratings, y = as.numeric(gsub("[^0-9]", "", ReviewCount)))) +
  geom_point(alpha = 0.6, color = "purple") +
  labs(title = "Correlation Between Ratings and Reviews", x = "Ratings", y = "Number of Reviews") +
  theme_minimal()

```
#9.

```{r}
library(ggplot2)

ggplot(all_products, aes(x = Price, y = Ratings, color = Category)) +
  geom_point() +
  facet_wrap(~ Category, scales = "free") +
  labs(title = "Ratings vs Price for Each Category",
       x = "Price ($)",
       y = "Ratings") +
  theme_minimal() +
  theme(legend.position = "none")
```
#10. Ranking by ratings lists products from highest to lowest, highlighting the best-reviewed items. Ranking by price, either from cheapest to most expensive or vice versa, helps users choose products within their budget.

```{r}

all_products <- read.csv("Amazon_products.csv")


str(all_products)

```

```{r}
# Load the CSV file containing the products
all_products <- read.csv("Amazon_products.csv")

# Ensure the Ratings column is numeric for sorting
all_products$Ratings <- as.numeric(all_products$Ratings)

# Check the structure to make sure Ratings are correctly formatted
str(all_products)

library(dplyr)

# Rank products by Ratings in descending order
ranked_by_ratings <- all_products %>%
  arrange(desc(Ratings))

# View the top-ranked products based on ratings
head(ranked_by_ratings, 150)  # Display the top 10 highest-rated products
```
```{r}
# Load the CSV file containing the products
all_products <- read.csv("Amazon_products.csv")

# Clean and convert the Price column to numeric (if it includes "$" signs)
all_products$Price <- as.numeric(gsub("\\$", "", all_products$Price))

# Check the structure to ensure Price is correctly formatted
str(all_products)

library(dplyr)

# Rank products by Price in ascending order (cheapest first)
ranked_by_price_ascending <- all_products %>%
  arrange(Price)

# Alternatively, rank products by Price in descending order (most expensive first)
ranked_by_price_descending <- all_products %>%
  arrange(desc(Price))

# View the top-ranked products based on price (descending order)
head(ranked_by_price_descending, 150)  # Display top 10 most expensive products
```

